{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Model Training and tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we finetune our LightGBM model using Optuna. The steps include:\n",
    "1. Loading preprocessed data.\n",
    "2. Loading configuration (the list of features to use) from our YAML file.\n",
    "3. Optionally performing an out-of-time split.\n",
    "4. Running hyperparameter optimization via Optuna.\n",
    "5. Saving the best model to the `artifacts/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.finetune_model import finetune_model\n",
    "from data.split_data import split_data_by_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (150000, 46)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"../data\", \"processed\", \"processed_data.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Original data shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants_path = os.path.join(os.getcwd(), \"../src/utils\", \"constants.yaml\")\n",
    "with open(constants_path, \"r\") as file:\n",
    "    constants = yaml.safe_load(file)\n",
    "FEATURES_TO_USE_AFTER_SELECTION = constants[\"FEATURES_TO_USE_AFTER_SELECTION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (90063, 16)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_by_time(\n",
    "    df\n",
    ")\n",
    "# Filter training set to keep only the selected features.\n",
    "X_train = X_train[FEATURES_TO_USE_AFTER_SELECTION]\n",
    "print(\"Training set shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 22:06:13,100] A new study created in memory with name: lgbm_finetuning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48d577b8e774a4e921053df8a76717c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-17 22:06:19,081] Trial 3 finished with value: 0.8490295742804053 and parameters: {'n_estimators': 242, 'max_depth': 3, 'learning_rate': 0.29181389848419775, 'subsample': 0.9838636166923819, 'colsample_bytree': 0.636832786230453, 'num_leaves': 33}. Best is trial 3 with value: 0.8490295742804053.\n",
      "[I 2025-02-17 22:06:22,102] Trial 1 finished with value: 0.8490804925958058 and parameters: {'n_estimators': 395, 'max_depth': 4, 'learning_rate': 0.13482210171397874, 'subsample': 0.6020050401047186, 'colsample_bytree': 0.7634442774076408, 'num_leaves': 32}. Best is trial 1 with value: 0.8490804925958058.\n",
      "[I 2025-02-17 22:06:31,246] Trial 5 finished with value: 0.8517812005266145 and parameters: {'n_estimators': 430, 'max_depth': 5, 'learning_rate': 0.09546021720973098, 'subsample': 0.5757371977858275, 'colsample_bytree': 0.9062438718039114, 'num_leaves': 19}. Best is trial 5 with value: 0.8517812005266145.\n",
      "[I 2025-02-17 22:06:32,620] Trial 0 finished with value: 0.8298860003615667 and parameters: {'n_estimators': 283, 'max_depth': 7, 'learning_rate': 0.23236638087000167, 'subsample': 0.5642966845204986, 'colsample_bytree': 0.7251309196349598, 'num_leaves': 51}. Best is trial 5 with value: 0.8517812005266145.\n",
      "[I 2025-02-17 22:06:35,088] Trial 2 finished with value: 0.8579703369381715 and parameters: {'n_estimators': 410, 'max_depth': 7, 'learning_rate': 0.018316149251476493, 'subsample': 0.7258321693391219, 'colsample_bytree': 0.6502922290823465, 'num_leaves': 45}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:06:37,800] Trial 7 finished with value: 0.8565719002402368 and parameters: {'n_estimators': 159, 'max_depth': 5, 'learning_rate': 0.07264260105664042, 'subsample': 0.6297994394325541, 'colsample_bytree': 0.5614900041392612, 'num_leaves': 67}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:06:38,771] Trial 8 finished with value: 0.8549625768650483 and parameters: {'n_estimators': 311, 'max_depth': 3, 'learning_rate': 0.1101523859415908, 'subsample': 0.8280112512154321, 'colsample_bytree': 0.9057794907794076, 'num_leaves': 60}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:06:39,491] Trial 9 finished with value: 0.8437717035656865 and parameters: {'n_estimators': 143, 'max_depth': 3, 'learning_rate': 0.01644618549297151, 'subsample': 0.7511271787695617, 'colsample_bytree': 0.8357095887376573, 'num_leaves': 108}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:06:47,882] Trial 11 finished with value: 0.8564710904134902 and parameters: {'n_estimators': 484, 'max_depth': 4, 'learning_rate': 0.04038781129394843, 'subsample': 0.5483407992341818, 'colsample_bytree': 0.7080516020527966, 'num_leaves': 112}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:06:51,874] Trial 10 finished with value: 0.8560940829215605 and parameters: {'n_estimators': 259, 'max_depth': 6, 'learning_rate': 0.018863814303473742, 'subsample': 0.9253089903722145, 'colsample_bytree': 0.8511087200597071, 'num_leaves': 51}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:06:54,378] Trial 12 finished with value: 0.8521185830483953 and parameters: {'n_estimators': 153, 'max_depth': 9, 'learning_rate': 0.13249681933548357, 'subsample': 0.57320070408752, 'colsample_bytree': 0.8215317992940769, 'num_leaves': 34}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:06,257] Trial 4 finished with value: 0.8523672759771369 and parameters: {'n_estimators': 423, 'max_depth': 9, 'learning_rate': 0.031116750347259317, 'subsample': 0.680926348925253, 'colsample_bytree': 0.8042833023103937, 'num_leaves': 108}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:07,759] Trial 6 finished with value: 0.8161120117576406 and parameters: {'n_estimators': 351, 'max_depth': 9, 'learning_rate': 0.28411370032844185, 'subsample': 0.9759010974696182, 'colsample_bytree': 0.833212927394281, 'num_leaves': 103}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:18,993] Trial 14 finished with value: 0.8559705139061122 and parameters: {'n_estimators': 339, 'max_depth': 8, 'learning_rate': 0.034166105996446636, 'subsample': 0.6900740884915549, 'colsample_bytree': 0.5000764102205741, 'num_leaves': 85}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:21,744] Trial 16 finished with value: 0.8554339227986706 and parameters: {'n_estimators': 196, 'max_depth': 7, 'learning_rate': 0.06506733047025559, 'subsample': 0.6975542754990971, 'colsample_bytree': 0.5139815717662254, 'num_leaves': 146}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:24,020] Trial 13 finished with value: 0.8563238531605789 and parameters: {'n_estimators': 370, 'max_depth': 10, 'learning_rate': 0.033783478618659595, 'subsample': 0.7217093049922624, 'colsample_bytree': 0.5129834655215807, 'num_leaves': 89}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:29,406] Trial 19 finished with value: 0.8528751320651791 and parameters: {'n_estimators': 102, 'max_depth': 6, 'learning_rate': 0.012811163405956656, 'subsample': 0.8112746612097319, 'colsample_bytree': 0.608435061200695, 'num_leaves': 69}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:29,990] Trial 17 finished with value: 0.8549547770004757 and parameters: {'n_estimators': 210, 'max_depth': 6, 'learning_rate': 0.01054475567439585, 'subsample': 0.7879836615972757, 'colsample_bytree': 0.567368285501451, 'num_leaves': 144}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:31,161] Trial 18 finished with value: 0.8568750946783785 and parameters: {'n_estimators': 205, 'max_depth': 6, 'learning_rate': 0.06102608971826028, 'subsample': 0.7836795373644347, 'colsample_bytree': 0.6022885920202256, 'num_leaves': 75}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:32,211] Trial 15 finished with value: 0.857909478689608 and parameters: {'n_estimators': 350, 'max_depth': 7, 'learning_rate': 0.01099768748198467, 'subsample': 0.691834906336265, 'colsample_bytree': 0.5112615430484959, 'num_leaves': 80}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:42,484] Trial 20 finished with value: 0.852481797912402 and parameters: {'n_estimators': 477, 'max_depth': 5, 'learning_rate': 0.06579795846852093, 'subsample': 0.6322879268740287, 'colsample_bytree': 0.602374098774708, 'num_leaves': 72}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:07:43,112] Trial 21 finished with value: 0.8570407953984974 and parameters: {'n_estimators': 499, 'max_depth': 5, 'learning_rate': 0.022261177504051873, 'subsample': 0.638504874041006, 'colsample_bytree': 0.6566984416609197, 'num_leaves': 71}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:08:04,744] Trial 25 finished with value: 0.8575926002739729 and parameters: {'n_estimators': 452, 'max_depth': 8, 'learning_rate': 0.02111789220750574, 'subsample': 0.509393720501332, 'colsample_bytree': 0.6834034140848951, 'num_leaves': 50}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:08:06,655] Trial 24 finished with value: 0.8573023496838135 and parameters: {'n_estimators': 318, 'max_depth': 8, 'learning_rate': 0.019525883486883246, 'subsample': 0.8925055279368819, 'colsample_bytree': 0.6479021840573839, 'num_leaves': 85}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:08:17,218] Trial 22 finished with value: 0.8539215924237586 and parameters: {'n_estimators': 492, 'max_depth': 8, 'learning_rate': 0.022768716230565752, 'subsample': 0.860916810515397, 'colsample_bytree': 0.6693969949745829, 'num_leaves': 129}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:08:18,021] Trial 23 finished with value: 0.854747349959044 and parameters: {'n_estimators': 496, 'max_depth': 8, 'learning_rate': 0.021859036257555083, 'subsample': 0.8727218360028001, 'colsample_bytree': 0.661026501865586, 'num_leaves': 130}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:08:25,267] Trial 26 finished with value: 0.8569834927178717 and parameters: {'n_estimators': 452, 'max_depth': 8, 'learning_rate': 0.02308468318005112, 'subsample': 0.5193043937529493, 'colsample_bytree': 0.6795579672718701, 'num_leaves': 48}. Best is trial 2 with value: 0.8579703369381715.\n",
      "[I 2025-02-17 22:08:27,936] Trial 27 finished with value: 0.8579724967598044 and parameters: {'n_estimators': 432, 'max_depth': 8, 'learning_rate': 0.012661505641065324, 'subsample': 0.5051124366920716, 'colsample_bytree': 0.6927611923304562, 'num_leaves': 49}. Best is trial 27 with value: 0.8579724967598044.\n",
      "[I 2025-02-17 22:08:37,066] Trial 31 finished with value: 0.8558099867003301 and parameters: {'n_estimators': 391, 'max_depth': 7, 'learning_rate': 0.013168676101574418, 'subsample': 0.7349934498416454, 'colsample_bytree': 0.9985966473098902, 'num_leaves': 16}. Best is trial 27 with value: 0.8579724967598044.\n",
      "[I 2025-02-17 22:08:39,452] Trial 28 finished with value: 0.8580159418292974 and parameters: {'n_estimators': 440, 'max_depth': 7, 'learning_rate': 0.01398007468350437, 'subsample': 0.724005908173589, 'colsample_bytree': 0.6914493402100474, 'num_leaves': 47}. Best is trial 28 with value: 0.8580159418292974.\n",
      "[I 2025-02-17 22:08:40,003] Trial 29 finished with value: 0.858032993309074 and parameters: {'n_estimators': 440, 'max_depth': 7, 'learning_rate': 0.013217624220241203, 'subsample': 0.5085807356104766, 'colsample_bytree': 0.7680140041838176, 'num_leaves': 47}. Best is trial 29 with value: 0.858032993309074.\n",
      "[I 2025-02-17 22:08:43,502] Trial 30 finished with value: 0.8570003976469686 and parameters: {'n_estimators': 397, 'max_depth': 7, 'learning_rate': 0.014369221471020628, 'subsample': 0.5008076318828124, 'colsample_bytree': 0.993709219169084, 'num_leaves': 42}. Best is trial 29 with value: 0.858032993309074.\n",
      "[I 2025-02-17 22:08:55,091] Trial 32 finished with value: 0.8560439418968299 and parameters: {'n_estimators': 279, 'max_depth': 7, 'learning_rate': 0.010594492571583443, 'subsample': 0.6590208644229081, 'colsample_bytree': 0.7469313443521695, 'num_leaves': 59}. Best is trial 29 with value: 0.858032993309074.\n",
      "[I 2025-02-17 22:08:57,377] Trial 34 finished with value: 0.8581083426255132 and parameters: {'n_estimators': 435, 'max_depth': 7, 'learning_rate': 0.015432854080346156, 'subsample': 0.5459175662086634, 'colsample_bytree': 0.7523972754168415, 'num_leaves': 41}. Best is trial 34 with value: 0.8581083426255132.\n",
      "[I 2025-02-17 22:09:01,018] Trial 33 finished with value: 0.857883545459031 and parameters: {'n_estimators': 408, 'max_depth': 10, 'learning_rate': 0.015480868461936128, 'subsample': 0.6534615934987673, 'colsample_bytree': 0.7423479216169272, 'num_leaves': 40}. Best is trial 34 with value: 0.8581083426255132.\n",
      "[I 2025-02-17 22:09:09,092] Trial 36 finished with value: 0.8576099534594427 and parameters: {'n_estimators': 412, 'max_depth': 7, 'learning_rate': 0.015163246978691336, 'subsample': 0.5977439894970366, 'colsample_bytree': 0.7687909846176638, 'num_leaves': 26}. Best is trial 34 with value: 0.8581083426255132.\n",
      "[I 2025-02-17 22:09:09,792] Trial 35 finished with value: 0.8575738763464701 and parameters: {'n_estimators': 420, 'max_depth': 7, 'learning_rate': 0.015945546397375526, 'subsample': 0.5429675742842601, 'colsample_bytree': 0.7390097265807396, 'num_leaves': 59}. Best is trial 34 with value: 0.8581083426255132.\n",
      "[I 2025-02-17 22:09:14,688] Trial 38 finished with value: 0.857061888708629 and parameters: {'n_estimators': 446, 'max_depth': 6, 'learning_rate': 0.027469157654561475, 'subsample': 0.5418887703267646, 'colsample_bytree': 0.774972048044377, 'num_leaves': 25}. Best is trial 34 with value: 0.8581083426255132.\n",
      "[I 2025-02-17 22:09:19,206] Trial 37 finished with value: 0.858312701821905 and parameters: {'n_estimators': 452, 'max_depth': 9, 'learning_rate': 0.015746356725623977, 'subsample': 0.5399095192309858, 'colsample_bytree': 0.7763365488261051, 'num_leaves': 38}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:24,026] Trial 39 finished with value: 0.8556707256880726 and parameters: {'n_estimators': 445, 'max_depth': 9, 'learning_rate': 0.04343230936949214, 'subsample': 0.5418497689163414, 'colsample_bytree': 0.7830887851840118, 'num_leaves': 30}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:30,664] Trial 40 finished with value: 0.8581225464747757 and parameters: {'n_estimators': 450, 'max_depth': 9, 'learning_rate': 0.025769349918741854, 'subsample': 0.5334962834713691, 'colsample_bytree': 0.7005273282034359, 'num_leaves': 34}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:33,095] Trial 41 finished with value: 0.854963306196867 and parameters: {'n_estimators': 384, 'max_depth': 9, 'learning_rate': 0.0482947881840608, 'subsample': 0.6101908562550382, 'colsample_bytree': 0.7117873244049351, 'num_leaves': 37}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:40,624] Trial 42 finished with value: 0.8551491788384367 and parameters: {'n_estimators': 470, 'max_depth': 9, 'learning_rate': 0.04512199165869644, 'subsample': 0.6065229691715947, 'colsample_bytree': 0.7928256018309098, 'num_leaves': 36}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:46,977] Trial 43 finished with value: 0.8575920794093597 and parameters: {'n_estimators': 469, 'max_depth': 9, 'learning_rate': 0.027162083503937347, 'subsample': 0.5991129889544016, 'colsample_bytree': 0.7235448917318625, 'num_leaves': 38}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:49,020] Trial 45 finished with value: 0.8578385097235465 and parameters: {'n_estimators': 469, 'max_depth': 10, 'learning_rate': 0.02675122397980864, 'subsample': 0.5952899075701734, 'colsample_bytree': 0.8005130472975762, 'num_leaves': 24}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:54,124] Trial 44 finished with value: 0.8575508512815082 and parameters: {'n_estimators': 469, 'max_depth': 9, 'learning_rate': 0.027921146622943916, 'subsample': 0.5900053483201608, 'colsample_bytree': 0.7087523293780479, 'num_leaves': 38}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:55,235] Trial 46 finished with value: 0.8575292974073279 and parameters: {'n_estimators': 465, 'max_depth': 10, 'learning_rate': 0.017636743074734087, 'subsample': 0.5742895316117033, 'colsample_bytree': 0.8758145012701315, 'num_leaves': 22}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:09:58,496] Trial 47 finished with value: 0.839238848829247 and parameters: {'n_estimators': 368, 'max_depth': 10, 'learning_rate': 0.19643623357414566, 'subsample': 0.5777036213782168, 'colsample_bytree': 0.8545779020869825, 'num_leaves': 25}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:10:06,378] Trial 48 finished with value: 0.8582975835232505 and parameters: {'n_estimators': 377, 'max_depth': 10, 'learning_rate': 0.0175104085591138, 'subsample': 0.5691740950445534, 'colsample_bytree': 0.7546186384177714, 'num_leaves': 60}. Best is trial 37 with value: 0.858312701821905.\n",
      "[I 2025-02-17 22:10:09,210] Trial 49 finished with value: 0.8581496351350608 and parameters: {'n_estimators': 376, 'max_depth': 10, 'learning_rate': 0.01726513871183949, 'subsample': 0.5311371632856701, 'colsample_bytree': 0.8810698245374379, 'num_leaves': 56}. Best is trial 37 with value: 0.858312701821905.\n",
      "Best Parameters: {'n_estimators': 452, 'max_depth': 9, 'learning_rate': 0.015746356725623977, 'subsample': 0.5399095192309858, 'colsample_bytree': 0.7763365488261051, 'num_leaves': 38}\n",
      "[LightGBM] [Info] Number of positive: 4607, number of negative: 85456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1975\n",
      "[LightGBM] [Info] Number of data points in the train set: 90063, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Best model saved to c:\\Users\\gusth\\Projetos\\case_meli\\marketplace_fraud_detection\\notebooks\\../artifacts\\best_model_1.pkl\n",
      "Best hyperparameters found:\n",
      "{'n_estimators': 452, 'max_depth': 9, 'learning_rate': 0.015746356725623977, 'subsample': 0.5399095192309858, 'colsample_bytree': 0.7763365488261051, 'num_leaves': 38, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "best_model, best_params = finetune_model(X_train, y_train, model_output_name=\"best_model_1.pkl\", n_trials=50)\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detector_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
